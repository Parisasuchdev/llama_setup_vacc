{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed LLM Disambiguation\n",
    "\n",
    "The LLM gave terrible reasoning: said \"McLuhan\" is a \"common noun\" and \"not a surname\" when comparing \"Marshall McLuhan\" vs \"McLuhan\".\n",
    "\n",
    "Let's fix this with a much more explicit prompt that forces correct reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/j/s/jstonge1/llama_setup_vacc/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Same imports\n",
    "from outlines import Generator, from_transformers, Template\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import sqlite3\n",
    "import json\n",
    "from typing import List, Optional\n",
    "from rich.console import Console\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schemas defined\n"
     ]
    }
   ],
   "source": [
    "# Same schemas\n",
    "class Person(BaseModel):\n",
    "    display_name: str = Field(description=\"The canonical name of the person.\")\n",
    "    display_name_alternatives: List[str] = Field(description=\"Other ways this person's name is displayed.\")\n",
    "\n",
    "class DisambiguationResponse(BaseModel):\n",
    "    same_person: bool = Field(description=\"Whether the two names refer to the same person\")\n",
    "    confidence: float = Field(description=\"Confidence score from 0.0 to 1.0\")\n",
    "    reasoning: str = Field(description=\"Brief explanation\")\n",
    "\n",
    "print(\"Schemas defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Much More Explicit Prompt\n",
    "\n",
    "Force the LLM to think step-by-step about surnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed LLM Disambiguator defined\n"
     ]
    }
   ],
   "source": [
    "class FixedLLMDisambiguator:\n",
    "    def __init__(self, model):\n",
    "        self.generator = Generator(model, DisambiguationResponse)\n",
    "        \n",
    "        # Much more explicit template\n",
    "        self.template = Template.from_string(\n",
    "            \"\"\"You are an academic name disambiguation expert.\n",
    "\n",
    "CRITICAL RULE: In academic writing, authors are first mentioned by full name, then by SURNAME ONLY.\n",
    "\n",
    "STEP-BY-STEP ANALYSIS:\n",
    "1. Extract the SURNAME (last word) from each name\n",
    "2. If surnames match AND one name is just the surname, they are the SAME PERSON\n",
    "3. Academic examples:\n",
    "   - \"Marshall McLuhan\" → surname is \"McLuhan\"\n",
    "   - \"McLuhan\" → this IS the surname \"McLuhan\"\n",
    "   - THEREFORE: \"Marshall McLuhan\" and \"McLuhan\" = SAME PERSON ✓\n",
    "\n",
    "MORE EXAMPLES:\n",
    "- \"Walter J. Ong\" + \"Ong\" = SAME PERSON (surname match)\n",
    "- \"Frank Kermode\" + \"Kermode\" = SAME PERSON (surname match)\n",
    "- \"John Smith\" + \"Jane Smith\" = DIFFERENT PEOPLE (same surname, different first names)\n",
    "- \"Plato\" + \"Aristotle\" = DIFFERENT PEOPLE (completely different names)\n",
    "\n",
    "Now analyze:\n",
    "\n",
    "NAME 1: {{ name1 }}\n",
    "CONTEXT 1: {{ context1 }}\n",
    "\n",
    "NAME 2: {{ name2 }}\n",
    "CONTEXT 2: {{ context2 }}\n",
    "\n",
    "ANALYSIS STEPS:\n",
    "1. What is the surname of NAME 1?\n",
    "2. What is the surname of NAME 2?\n",
    "3. Are the surnames the same?\n",
    "4. Is one name just the surname of the other?\n",
    "5. Do contexts suggest same academic person?\n",
    "\n",
    "If surnames match and one is just the surname, they are the SAME PERSON.\n",
    "\n",
    "RESPONSE:\n",
    "\"\"\")\n",
    "    \n",
    "    def are_same_person(self, name1: str, context1: str, name2: str, context2: str):\n",
    "        prompt = self.template(\n",
    "            name1=name1,\n",
    "            context1=context1[:200],\n",
    "            name2=name2,\n",
    "            context2=context2[:200]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            result = self.generator(prompt, max_new_tokens=300, temperature=0.0, do_sample=False)\n",
    "            return json.loads(result)\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"same_person\": False,\n",
    "                \"confidence\": 0.0,\n",
    "                \"reasoning\": f\"Error: {e}\"\n",
    "            }\n",
    "\n",
    "print(\"Fixed LLM Disambiguator defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Fixed Approach\n",
    "\n",
    "See if the step-by-step analysis fixes the terrible reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_path = \"/gpfs1/llm/llama-3.2-hf/Meta-Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model = from_transformers(\n",
    "    AutoModelForCausalLM.from_pretrained(model_path, device_map=\"cuda\"),\n",
    "    AutoTokenizer.from_pretrained(model_path)\n",
    ")\n",
    "\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Testing FIXED LLM disambiguation:</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTesting FIXED LLM disambiguation:\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Test </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">: Marshall McLuhan vs McLuhan</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTest \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m: Marshall McLuhan vs McLuhan\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/j/s/jstonge1/llama_setup_vacc/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/users/j/s/jstonge1/llama_setup_vacc/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">SHOULD MATCH </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">same person - surname pattern</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mSHOULD MATCH \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32msame person - surname pattern\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">LLM Decision: </span><span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">True</span><span style=\"color: #008000; text-decoration-color: #008000\"> </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">confidence: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.90</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mLLM Decision: \u001b[0m\u001b[3;32mTrue\u001b[0m\u001b[32m \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mconfidence: \u001b[0m\u001b[1;32m0.90\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Expected: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span> | Correct: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Expected: \u001b[3;92mTrue\u001b[0m | Correct: \u001b[3;92mTrue\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Reasoning:</span> The surnames match and McLuhan is just the surname of Marshall McLuhan, indicating they are the same \n",
       "person.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mReasoning:\u001b[0m The surnames match and McLuhan is just the surname of Marshall McLuhan, indicating they are the same \n",
       "person.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Test </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">: Walter J. Ong vs Ong</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTest \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m: Walter J. Ong vs Ong\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/j/s/jstonge1/llama_setup_vacc/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/users/j/s/jstonge1/llama_setup_vacc/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">SHOULD MATCH </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">same person - surname pattern</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mSHOULD MATCH \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32msame person - surname pattern\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">LLM Decision: </span><span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">True</span><span style=\"color: #008000; text-decoration-color: #008000\"> </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">confidence: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.90</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mLLM Decision: \u001b[0m\u001b[3;32mTrue\u001b[0m\u001b[32m \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mconfidence: \u001b[0m\u001b[1;32m0.90\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Expected: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span> | Correct: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Expected: \u001b[3;92mTrue\u001b[0m | Correct: \u001b[3;92mTrue\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Reasoning:</span> The surnames match <span style=\"font-weight: bold\">(</span>Ong<span style=\"font-weight: bold\">)</span> and NAME <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> is a full name <span style=\"font-weight: bold\">(</span>Walter J. Ong<span style=\"font-weight: bold\">)</span>, which suggests that NAME <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> is the \n",
       "full name of the person referred to by NAME <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. The contexts also suggest that NAME <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> is the academic person being \n",
       "referred to, as NAME <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> is described as a Jesuit priest and philosopher, while NAME <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> is described as noting \n",
       "something. This high confidence level is due to the fact that the contexts strongly suggest that NAME <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> is the \n",
       "person being referred to, and the matching surnames provide strong evidence of the same person.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mReasoning:\u001b[0m The surnames match \u001b[1m(\u001b[0mOng\u001b[1m)\u001b[0m and NAME \u001b[1;36m1\u001b[0m is a full name \u001b[1m(\u001b[0mWalter J. Ong\u001b[1m)\u001b[0m, which suggests that NAME \u001b[1;36m1\u001b[0m is the \n",
       "full name of the person referred to by NAME \u001b[1;36m2\u001b[0m. The contexts also suggest that NAME \u001b[1;36m1\u001b[0m is the academic person being \n",
       "referred to, as NAME \u001b[1;36m1\u001b[0m is described as a Jesuit priest and philosopher, while NAME \u001b[1;36m2\u001b[0m is described as noting \n",
       "something. This high confidence level is due to the fact that the contexts strongly suggest that NAME \u001b[1;36m1\u001b[0m is the \n",
       "person being referred to, and the matching surnames provide strong evidence of the same person.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Test </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">: Frank Kermode vs Kermode</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTest \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m: Frank Kermode vs Kermode\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">SHOULD MATCH </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">same person - surname pattern</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mSHOULD MATCH \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32msame person - surname pattern\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">LLM Decision: </span><span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">True</span><span style=\"color: #008000; text-decoration-color: #008000\"> </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">confidence: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">100.00</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mLLM Decision: \u001b[0m\u001b[3;32mTrue\u001b[0m\u001b[32m \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mconfidence: \u001b[0m\u001b[1;32m100.00\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Expected: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span> | Correct: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Expected: \u001b[3;92mTrue\u001b[0m | Correct: \u001b[3;92mTrue\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Reasoning:</span> The surnames match and one name is just the surname of the other, suggesting they are the same academic \n",
       "person.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mReasoning:\u001b[0m The surnames match and one name is just the surname of the other, suggesting they are the same academic \n",
       "person.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Test </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">: Plato vs Socrates</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTest \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m: Plato vs Socrates\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Should NOT match </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">different people entirely</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mShould NOT match \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mdifferent people entirely\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">LLM Decision: </span><span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">False</span><span style=\"color: #008000; text-decoration-color: #008000\"> </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">confidence: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.00</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mLLM Decision: \u001b[0m\u001b[3;32mFalse\u001b[0m\u001b[32m \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mconfidence: \u001b[0m\u001b[1;32m0.00\u001b[0m\u001b[1;32m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Expected: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span> | Correct: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Expected: \u001b[3;91mFalse\u001b[0m | Correct: \u001b[3;92mTrue\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Reasoning:</span> Error: Unterminated string starting at: line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> column <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56</span> <span style=\"font-weight: bold\">(</span>char <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mReasoning:\u001b[0m Error: Unterminated string starting at: line \u001b[1;36m1\u001b[0m column \u001b[1;36m56\u001b[0m \u001b[1m(\u001b[0mchar \u001b[1;36m55\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">🎉 ALL TESTS PASSED! Fixed disambiguation working correctly.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m🎉 ALL TESTS PASSED! Fixed disambiguation working correctly.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the fixed disambiguator\n",
    "fixed_disambiguator = FixedLLMDisambiguator(model)\n",
    "console = Console()\n",
    "\n",
    "# Focus on the cases that were failing\n",
    "test_cases = [\n",
    "    {\n",
    "        'name1': 'Marshall McLuhan',\n",
    "        'context1': 'Like Marshall McLuhan, with whom he was compared',\n",
    "        'name2': 'McLuhan',\n",
    "        'context2': 'Whether McLuhan would have seen cyberspace',\n",
    "        'expected': True,\n",
    "        'label': 'SHOULD MATCH (same person - surname pattern)'\n",
    "    },\n",
    "    {\n",
    "        'name1': 'Walter J. Ong',\n",
    "        'context1': 'proposed Walter J. Ong, Jesuit priest, philosopher',\n",
    "        'name2': 'Ong',\n",
    "        'context2': 'As Ong noted, the expression to look up something',\n",
    "        'expected': True,\n",
    "        'label': 'SHOULD MATCH (same person - surname pattern)'\n",
    "    },\n",
    "    {\n",
    "        'name1': 'Frank Kermode',\n",
    "        'context1': 'said a scornful Frank Kermode',\n",
    "        'name2': 'Kermode',\n",
    "        'context2': 'Kermode criticized the approach',\n",
    "        'expected': True,\n",
    "        'label': 'SHOULD MATCH (same person - surname pattern)'\n",
    "    },\n",
    "    {\n",
    "        'name1': 'Plato',\n",
    "        'context1': 'Plato warned that this technology meant impoverishment',\n",
    "        'name2': 'Socrates',\n",
    "        'context2': 'channeling the nonwriter Socrates',\n",
    "        'expected': False,\n",
    "        'label': 'Should NOT match (different people entirely)'\n",
    "    }\n",
    "]\n",
    "\n",
    "console.print(\"[bold]Testing FIXED LLM disambiguation:[/bold]\\n\")\n",
    "\n",
    "all_correct = True\n",
    "\n",
    "for i, test in enumerate(test_cases):\n",
    "    console.print(f\"[bold]Test {i+1}: {test['name1']} vs {test['name2']}[/bold]\")\n",
    "    \n",
    "    decision = fixed_disambiguator.are_same_person(\n",
    "        test['name1'], test['context1'],\n",
    "        test['name2'], test['context2']\n",
    "    )\n",
    "    \n",
    "    correct = decision['same_person'] == test['expected']\n",
    "    all_correct = all_correct and correct\n",
    "    \n",
    "    color = \"green\" if correct else \"red\"\n",
    "    console.print(f\"[{color}]{test['label']}[/{color}]\")\n",
    "    console.print(f\"[{color}]LLM Decision: {decision['same_person']} (confidence: {decision['confidence']:.2f})[/{color}]\")\n",
    "    console.print(f\"Expected: {test['expected']} | Correct: {correct}\")\n",
    "    console.print(f\"[bold]Reasoning:[/bold] {decision['reasoning']}\")\n",
    "    console.print()\n",
    "\n",
    "if all_correct:\n",
    "    console.print(\"[bold green]🎉 ALL TESTS PASSED! Fixed disambiguation working correctly.[/bold green]\")\n",
    "else:\n",
    "    console.print(\"[bold red]❌ Some tests still failing. Need further prompt refinement.[/bold red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Simple Rule-Based Fallback\n",
    "\n",
    "If LLM still fails, use simple rule as backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedBackup:\n",
    "    def surname_match_check(self, name1: str, name2: str) -> bool:\n",
    "        \"\"\"Simple rule: if one name is the surname of the other, they match\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Get last word of each name (likely surname)\n",
    "        surname1 = name1.strip().split()[-1]\n",
    "        surname2 = name2.strip().split()[-1]\n",
    "        \n",
    "        # Case 1: Exact surname match and one is just the surname\n",
    "        if surname1.lower() == surname2.lower():\n",
    "            # Check if one name is just the surname (2 words vs 1 word)\n",
    "            words1 = len(name1.strip().split())\n",
    "            words2 = len(name2.strip().split())\n",
    "            \n",
    "            if (words1 > 1 and words2 == 1) or (words1 == 1 and words2 > 1):\n",
    "                return True\n",
    "        \n",
    "        # Case 2: One name contains the other as a word boundary\n",
    "        if len(name1) > len(name2):\n",
    "            longer, shorter = name1, name2\n",
    "        else:\n",
    "            longer, shorter = name2, name1\n",
    "        \n",
    "        if len(shorter) > 2:\n",
    "            pattern = r'\\b' + re.escape(shorter.lower()) + r'\\b'\n",
    "            if re.search(pattern, longer.lower()):\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "# Test the rule-based approach\n",
    "rule_checker = RuleBasedBackup()\n",
    "\n",
    "console.print(\"\\n[bold]Testing simple rule-based backup:[/bold]\\n\")\n",
    "\n",
    "for test in test_cases:\n",
    "    rule_result = rule_checker.surname_match_check(test['name1'], test['name2'])\n",
    "    correct = rule_result == test['expected']\n",
    "    \n",
    "    color = \"green\" if correct else \"red\"\n",
    "    console.print(f\"[{color}]{test['name1']} vs {test['name2']}[/{color}]\")\n",
    "    console.print(f\"[{color}]Rule says: {rule_result} | Expected: {test['expected']} | Correct: {correct}[/{color}]\")\n",
    "    console.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid: LLM + Rule Backup\n",
    "\n",
    "Use rule-based check as fallback if LLM gives wrong answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridDisambiguator:\n",
    "    def __init__(self, model):\n",
    "        self.llm_disambiguator = FixedLLMDisambiguator(model)\n",
    "        self.rule_checker = RuleBasedBackup()\n",
    "    \n",
    "    def are_same_person(self, name1: str, context1: str, name2: str, context2: str):\n",
    "        # Try LLM first\n",
    "        llm_decision = self.llm_disambiguator.are_same_person(name1, context1, name2, context2)\n",
    "        \n",
    "        # Check if rule-based approach disagrees\n",
    "        rule_result = self.rule_checker.surname_match_check(name1, name2)\n",
    "        \n",
    "        # If LLM and rule agree, trust LLM\n",
    "        if llm_decision['same_person'] == rule_result:\n",
    "            llm_decision['method'] = 'llm_and_rule_agree'\n",
    "            return llm_decision\n",
    "        \n",
    "        # If they disagree, check if rule catches obvious surname pattern\n",
    "        if rule_result and not llm_decision['same_person']:\n",
    "            # Rule says match, LLM says no match - for surname patterns, trust rule\n",
    "            return {\n",
    "                'same_person': True,\n",
    "                'confidence': 0.9,\n",
    "                'reasoning': f'LLM said no ({llm_decision[\"reasoning\"][:50]}...) but rule detected obvious surname pattern',\n",
    "                'method': 'rule_override'\n",
    "            }\n",
    "        else:\n",
    "            # Trust LLM in other cases\n",
    "            llm_decision['method'] = 'llm_preferred'\n",
    "            return llm_decision\n",
    "\n",
    "# Test hybrid approach\n",
    "hybrid = HybridDisambiguator(model)\n",
    "\n",
    "console.print(\"\\n[bold]Testing HYBRID approach (LLM + rule backup):[/bold]\\n\")\n",
    "\n",
    "all_correct_hybrid = True\n",
    "\n",
    "for test in test_cases:\n",
    "    decision = hybrid.are_same_person(\n",
    "        test['name1'], test['context1'],\n",
    "        test['name2'], test['context2']\n",
    "    )\n",
    "    \n",
    "    correct = decision['same_person'] == test['expected']\n",
    "    all_correct_hybrid = all_correct_hybrid and correct\n",
    "    \n",
    "    color = \"green\" if correct else \"red\"\n",
    "    console.print(f\"[{color}]{test['name1']} vs {test['name2']}[/{color}]\")\n",
    "    console.print(f\"[{color}]Hybrid Decision: {decision['same_person']} (confidence: {decision['confidence']:.2f})[/{color}]\")\n",
    "    console.print(f\"Method: {decision['method']} | Expected: {test['expected']} | Correct: {correct}\")\n",
    "    console.print(f\"Reasoning: {decision['reasoning']}\")\n",
    "    console.print()\n",
    "\n",
    "if all_correct_hybrid:\n",
    "    console.print(\"[bold green]🎉 HYBRID APPROACH WORKS! All tests passed.[/bold green]\")\n",
    "else:\n",
    "    console.print(\"[bold yellow]⚠️ Hybrid needs more work.[/bold yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Fixing the LLM's Bad Reasoning\n",
    "\n",
    "The original LLM reasoning was terrible:\n",
    "> \"McLuhan is a common noun... not a surname... different people\"\n",
    "\n",
    "### 🔧 **Fixes Applied**\n",
    "1. **Step-by-step analysis** - force LLM to think about surnames explicitly\n",
    "2. **Explicit rules** - \"If surnames match AND one is just the surname, SAME PERSON\"\n",
    "3. **Multiple examples** - show the pattern clearly\n",
    "4. **Rule-based backup** - catch obvious cases if LLM still fails\n",
    "5. **Hybrid approach** - use rule override for surname patterns\n",
    "\n",
    "### 🎯 **Expected Results**\n",
    "- \"Marshall McLuhan\" and \"McLuhan\" should now correctly match\n",
    "- \"Walter J. Ong\" and \"Ong\" should correctly match\n",
    "- Better reasoning from the LLM about surname patterns\n",
    "- Rule-based backup catches cases where LLM reasoning fails\n",
    "\n",
    "### 💡 **Key Insight**\n",
    "Sometimes LLMs need **very explicit step-by-step instructions** for what seems obvious to humans. The academic citation pattern (full name → surname) needed to be spelled out completely.\n",
    "\n",
    "If the fixed prompt still doesn't work, the hybrid approach with rule-based backup should catch the obvious surname matches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-setup-vacc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
